{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "camera_cone_detection.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3i3JABp6XVPU",
        "D_0Ud59-QO2t",
        "EloEarf7VhBV",
        "nPxg_OD_yvwH",
        "EX6rTYFRqc5j",
        "-Ff4usNCqBQN"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svqgPR-uyZsY"
      },
      "source": [
        "# **StereoCamera cone detection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzIHYzwWymd2"
      },
      "source": [
        "## **Setup**\n",
        "\n",
        "Setup the repo and the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3i3JABp6XVPU"
      },
      "source": [
        "### **Clone the repositories and import the libraries**\n",
        "\n",
        "Clone the repository from the smart application course organization and the one for the yolov5 model and import the libraries needed to work with the porject."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lFVzjLYHpRh",
        "outputId": "a7c26f08-a1c8-4c2b-f8e8-438fdb95bfd1"
      },
      "source": [
        "!git clone --recurse-submodules -b dev_Petix_Ristori \"https://github.com/unipi-smartapp-2021/lidar-cone-detection\"   # clone our repository\n",
        "!git clone \"https://github.com/ultralytics/yolov5\"  # clone the model repository"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'lidar-cone-detection' already exists and is not an empty directory.\n",
            "fatal: destination path 'yolov5' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNLR_IusVerA"
      },
      "source": [
        "Install the requirements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtoC7WcCHyM6"
      },
      "source": [
        "!pip install -r lidar-cone-detection/requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQ7gSjz9QLFw"
      },
      "source": [
        "Import the libraries and mount the drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-2P-LryDj9R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4f2fdaf-0272-47ad-cf47-0e31dcff89a6"
      },
      "source": [
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import py7zr\n",
        "import zipfile\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')  # mount the drive"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_0Ud59-QO2t"
      },
      "source": [
        "### **Manage the dataset**\n",
        "\n",
        "There are two datasets, choose wisely:  \n",
        "- the first dataset is the small one and contains 101 images.\n",
        "- the second one is the full one with more than 8000 images.\n",
        "- there will be more datasets in the future."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gE1eopTKRpLs"
      },
      "source": [
        "def extract7z(zip_path):\n",
        "    with py7zr.SevenZipFile(zip_path, mode='r') as z:\n",
        "        z.extractall()\n",
        "\n",
        "def extractzip(zip_path):\n",
        "    with zipfile.ZipFile(zip_path, mode='r') as z:\n",
        "        z.extractall()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyrlPZGpQJnk"
      },
      "source": [
        "datasets = [\"/content/drive/Shareddrives/Sensory_data/ConeDataset.7z\",\n",
        "            \"/content/drive/Shareddrives/Sensory_data/TRset-20211117T155203Z-001.zip\"]\n",
        "datasets_extract = [extract7z, extractzip]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "019-VzEdR2Rh"
      },
      "source": [
        "datasets_extract[0](datasets[0])"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6nbWSNqR0bf"
      },
      "source": [
        "**Extract the chosen dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjszdXIf1af-"
      },
      "source": [
        "Manage the dataset by following what is suggested in https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data chapter 1.3. Keep in mind to modify the directory from which you're moving the images (ConeDataset for the first dataset or TRset for the full one).\n",
        "\n",
        "You can decide to convert the images to grayscale ones or not, then the dataset is splited into train, validation and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ofhspN2589x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "665fb785-8060-4406-a66f-4ed5b47dafee"
      },
      "source": [
        "!python lidar-cone-detection/src/manage_dataset.py --dataset ConeDataset --path lidar-cone-detection/src/datasets/ConeDataset"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: manage_dataset.py [-h] [--dataset DATASET] [--path PATH] [--m M]\n",
            "                         [--grayscale] [--split SPLIT [SPLIT ...]]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --dataset DATASET     dataset path\n",
            "  --path PATH           path where the dataset is going to be stored\n",
            "  --m M                 move or copy the dataset\n",
            "  --grayscale           convert the dataset images to grayscale\n",
            "  --split SPLIT [SPLIT ...]\n",
            "                        split the dataset into training, validation and test\n",
            "                        sets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EloEarf7VhBV"
      },
      "source": [
        "### **Check the yolov5 model and import the weights**\n",
        "\n",
        "Check if yolov5 is ready and fine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqN-E90RIJIn",
        "outputId": "6dde50e3-557f-43dd-e9c8-b80a6e8f3b39"
      },
      "source": [
        "%cd yolov5\n",
        "from yolov5.utils import notebook_init\n",
        "display = notebook_init()  # checks\n",
        "%cd ../"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 ðŸš€ v6.0-120-g92a7391 torch 1.10.0+cu111 CPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete âœ…\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th1yO-ZIRQqN"
      },
      "source": [
        "Import the yolov5 model, all the available models can be found here: https://github.com/ultralytics/yolov5/releases/."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wd0dUn1QWhH_",
        "outputId": "d3c8a1f6-071b-4fda-9a5d-3507bf58b428"
      },
      "source": [
        "import requests\n",
        "url = 'https://github.com/ultralytics/yolov5/releases/download/v6.0/yolov5s.pt'\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "open('yolov5s.pt', 'wb').write(r.content)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14698491"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPxg_OD_yvwH"
      },
      "source": [
        "## **Train the model**\n",
        "\n",
        "Train the model following the guide from https://github.com/ultralytics/yolov5/blob/master/tutorial.ipynb."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2ZTpdvIrV13"
      },
      "source": [
        "**Main arguments:**\n",
        "*   *--img*, image size.\n",
        "*   *--batch*, batch size.\n",
        "*   *--epochs*, number of epochs.\n",
        "*   *--data*, the yaml file that specifies where datasets are.\n",
        "*   *--weights*, the model weights (you can find them here https://github.com/ultralytics/yolov5/releases).\n",
        "*   *--cache*, to save a cache file of the train and validation sets.\n",
        "*   *--project*, where to save the results.\n",
        "\n",
        "**An example of train call would be:**\n",
        "\n",
        "!python yolov5/train.py --img img_size --batch batch_size --epochs epochs --data dataset.yaml --weights model --cache --project runs/train\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXHb1S1INMPi"
      },
      "source": [
        "!python yolov5/train.py --img 640 --batch 8 --epochs 100 --data lidar-cone-detection/src/stereocamera/conedataset.yaml --weights yolov5s.pt --cache --project runs/train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2W94swlV2aS"
      },
      "source": [
        "Use the tensorboard to see the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjflIYvMNEej"
      },
      "source": [
        "# Tensorboard  (optional)\n",
        "%reload_ext tensorboard\n",
        "#%load_ext tensorboard\n",
        "%tensorboard --logdir runs/train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EX6rTYFRqc5j"
      },
      "source": [
        "## **Evaluate the model**\n",
        "\n",
        "Use val.py if you have a test set defined in conedataset.yaml. Remember to use --task test or the model will work on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SY6M2bbEov8A",
        "outputId": "a5a40726-f2d4-4bc1-bdae-f09d6c383190"
      },
      "source": [
        "!python yolov5/val.py --weights /content/runs/train/exp2/weights/best.pt --img 640 --conf 0.5 --data lidar-cone-detection/src/stereocamera/conedataset.yaml --task test --project runs/test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mdata=lidar-cone-detection/src/stereocamera/conedataset.yaml, weights=['/content/drive/Shareddrives/Sensory_data/best.pt'], batch_size=32, imgsz=640, conf_thres=0.5, iou_thres=0.6, task=test, device=, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/test, name=exp, exist_ok=False, half=False, dnn=False\n",
            "WARNING: confidence threshold 0.5 >> 0.001 will produce invalid mAP values.\n",
            "YOLOv5 ðŸš€ v6.0-115-gbc48457 torch 1.10.0+cu111 CUDA:0 (Tesla K80, 11441MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 213 layers, 7020913 parameters, 0 gradients, 15.8 GFLOPs\n",
            "\u001b[34m\u001b[1mtest: \u001b[0mScanning 'lidar-cone-detection/src/stereocamera/datasets/ConeDataset/test' images and labels...11 found, 0 missing, 0 empty, 0 corrupted: 100% 11/11 [00:00<00:00, 469.75it/s]\n",
            "\u001b[34m\u001b[1mtest: \u001b[0mNew cache created: lidar-cone-detection/src/stereocamera/datasets/ConeDataset/test.cache\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:01<00:00,  1.50s/it]\n",
            "                 all         11        127      0.722      0.475      0.604      0.451\n",
            "                 big         11          4          0          0          0          0\n",
            "              orange         11          7          1      0.571      0.786      0.633\n",
            "              yellow         11         71      0.887      0.662      0.795      0.544\n",
            "                blue         11         45          1      0.667      0.833      0.628\n",
            "Speed: 0.3ms pre-process, 18.8ms inference, 1.8ms NMS per image at shape (32, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/test/exp\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ff4usNCqBQN"
      },
      "source": [
        "## **Inference**\n",
        "\n",
        "Use detect.py if you have no labeled images or you want to try the model on your custom data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ebRgAKBMBP7"
      },
      "source": [
        "!rm -rf detect/exp\n",
        "!python yolov5/detect.py --weights drive/Shareddrives/Sensory_data/best.pt --img 640 --conf 0.5 --source lidar-cone-detection/src/datasets/ConeDataset/images/image348.jpg --exist-ok --save-txt --project detect --hide-labels  --hide-conf"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}